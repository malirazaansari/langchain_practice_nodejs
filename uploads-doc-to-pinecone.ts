import * as dotenv from "dotenv";
dotenv.config();

import fs from "fs/promises";
import path from "path";
import { JSDOM } from "jsdom";
import { Pinecone } from "@pinecone-database/pinecone";
import { OpenAIEmbeddings } from "@langchain/openai";
import { PineconeStore } from "@langchain/pinecone";

let pdf: any;
async function loadPdfModule() {
  if (!pdf) {
    const mod = await import("pdf-parse");
    pdf = mod.default || mod;
  }
  return pdf;
}

const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const PINECONE_API_KEY = process.env.PINECONE_API_KEY;
const INDEX_NAME = process.env.PINECONE_INDEX || "rag-scanner";

// init clients
const embeddings = new OpenAIEmbeddings({
  apiKey: OPENAI_API_KEY,
  model: "text-embedding-3-small",
} as any);

const pinecone = new Pinecone({ apiKey: PINECONE_API_KEY } as any) as any;
const index = pinecone.index(INDEX_NAME) as any;

const CHUNK_SIZE = 800;

// üß© Text extractors
async function extractTextFromFile(filePath: string): Promise<string> {
  const ext = path.extname(filePath).toLowerCase();

  try {
    if (ext === ".pdf") {
      const buffer = await fs.readFile(filePath);
      const pdfModule = await loadPdfModule();
      const { text } = await pdfModule(buffer);
      return text;
    } else if (ext === ".html" || ext === ".htm") {
      const html = await fs.readFile(filePath, "utf8");
      const dom = new JSDOM(html);
      return dom.window.document.body.textContent || "";
    } else if (
      [".txt", ".md", ".yml", ".yaml", ".json", ".csv"].includes(ext)
    ) {
      return await fs.readFile(filePath, "utf8");
    } else {
      console.log(`‚ö†Ô∏è Skipping unsupported file: ${filePath}`);
      return "";
    }
  } catch (err: any) {
    console.error(`‚ùå Error reading ${filePath}:`, err.message);
    return "";
  }
}

// üß© Split text
function chunkText(text: string, size = CHUNK_SIZE) {
  const chunks: string[] = [];
  for (let i = 0; i < text.length; i += size) {
    const chunk = text.slice(i, i + size).trim();
    if (chunk.length > 50) chunks.push(chunk);
  }
  return chunks;
}

// üß† Skip already-uploaded files
async function alreadyUploaded(filename: string) {
  try {
    const result = await index.query({
      vector: Array(512).fill(0),
      filter: { source: filename },
      topK: 1,
      includeMetadata: true,
    } as any);
    return result.matches?.length > 0;
  } catch (err: any) {
    console.warn(`‚ö†Ô∏è Could not check ${filename}: ${err.message}`);
    return false;
  }
}

// üìÇ Recursively collect documents
async function findDocuments(dir: string) {
  const files: string[] = [];
  const entries = await fs.readdir(dir, { withFileTypes: true });
  for (const entry of entries) {
    const full = path.join(dir, entry.name);
    if (entry.isDirectory()) files.push(...(await findDocuments(full)));
    else if (/\.(pdf|txt|md|yaml|yml|json|html|htm|csv)$/i.test(entry.name))
      files.push(full);
  }
  return files;
}

// üöÄ Main upload
export async function uploadDocs() {
  console.log("üöÄ Starting document ingestion...");

  const docsDir = path.resolve("./docs");
  const files = await findDocuments(docsDir);

  console.log(`üìÅ Found ${files.length} documents in ${docsDir}`);

  for (const file of files) {
    const filename = path.basename(file);
    if (await alreadyUploaded(filename)) {
      console.log(`‚è≠Ô∏è Skipping ${filename} (already in Pinecone)`);
      continue;
    }

    const text = await extractTextFromFile(file);
    if (!text || text.length < 100) {
      console.log(`‚ö†Ô∏è Skipping ${filename} (empty or too short)`);
      continue;
    }

    const chunks = chunkText(text);

    const docs = chunks.map((content, i) => ({
      pageContent: content,
      metadata: {
        source: filename,
        fullPath: file,
        chunkIndex: i,
      },
    }));

    console.log(
      `üß† Creating embeddings for ${filename} (${chunks.length} chunks)`
    );

    await PineconeStore.fromDocuments(docs as any, embeddings as any, {
      pineconeIndex: index as any,
    } as any);

    console.log(`‚úÖ Uploaded ${filename} successfully`);
  }

  console.log("üéâ All documents processed and uploaded!");
}

// üß© Process a single document
async function processDocument(filePath: string) {
  const filename = path.basename(filePath);

  if (await alreadyUploaded(filename)) {
    console.log(`‚è≠Ô∏è Skipping ${filename} (already in Pinecone)`);
    return;
  }

  const text = await extractTextFromFile(filePath);
  if (!text || text.length < 100) {
    console.log(`‚ö†Ô∏è Skipping ${filename} (empty or too short)`);
    return;
  }

  const chunks = chunkText(text);

  const docs = chunks.map((content, i) => ({
    pageContent: content,
    metadata: {
      source: filename,
      fullPath: filePath,
      chunkIndex: i,
    },
  }));

  console.log(
    `üß† Creating embeddings for ${filename} (${chunks.length} chunks)`
  );

  await PineconeStore.fromDocuments(docs as any, embeddings as any, {
    pineconeIndex: index as any,
  } as any);

  console.log(`‚úÖ Uploaded ${filename} successfully`);
}

export async function uploadSingleFile(file: { originalname: string; buffer: Buffer }) {
  const tempPath = `/tmp/${file.originalname}`;
  await fs.writeFile(tempPath, file.buffer);
  await processDocument(tempPath);
  return { filename: file.originalname };
}

export async function uploadDocumentsFromDir(dirPath = "./docs") {
  const docs = await findDocuments(dirPath);
  for (const doc of docs) {
    await processDocument(doc);
  }
  return { total: docs.length };
}

// If executed directly, run the upload
if (require.main === module) {
  uploadDocs().catch((err) => console.error("‚ùå Ingestion error:", err));
}

export default uploadDocs;
